{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tf2 的一些demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "num_episodes = 500              # 游戏训练的总episode数量\n",
    "num_exploration_episodes = 100  # 探索过程所占的episode数量\n",
    "max_len_episode = 1000          # 每个episode的最大回合数\n",
    "batch_size = 32                 # 批次大小\n",
    "learning_rate = 1e-3            # 学习率\n",
    "gamma = 1.                      # 折扣因子\n",
    "initial_epsilon = 1.            # 探索起始时的探索率\n",
    "final_epsilon = 0.01            # 探索终止时的探索率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        q_values = self(inputs)\n",
    "        return tf.argmax(q_values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremy/anaconda/envs/tf2/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, epsilon 1.000000, score 38\n",
      "episode 1, epsilon 0.990000, score 14\n",
      "episode 2, epsilon 0.980000, score 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0221 15:38:02.449100 4563113408 base_layer.py:1790] Layer q_network is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3, epsilon 0.970000, score 17\n",
      "episode 4, epsilon 0.960000, score 11\n",
      "episode 5, epsilon 0.950000, score 29\n",
      "episode 6, epsilon 0.940000, score 13\n",
      "episode 7, epsilon 0.930000, score 12\n",
      "episode 8, epsilon 0.920000, score 13\n",
      "episode 9, epsilon 0.910000, score 31\n",
      "episode 10, epsilon 0.900000, score 32\n",
      "episode 11, epsilon 0.890000, score 12\n",
      "episode 12, epsilon 0.880000, score 36\n",
      "episode 13, epsilon 0.870000, score 17\n",
      "episode 14, epsilon 0.860000, score 37\n",
      "episode 15, epsilon 0.850000, score 18\n",
      "episode 16, epsilon 0.840000, score 12\n",
      "episode 17, epsilon 0.830000, score 20\n",
      "episode 18, epsilon 0.820000, score 11\n",
      "episode 19, epsilon 0.810000, score 10\n",
      "episode 20, epsilon 0.800000, score 18\n",
      "episode 21, epsilon 0.790000, score 15\n",
      "episode 22, epsilon 0.780000, score 31\n",
      "episode 23, epsilon 0.770000, score 17\n",
      "episode 24, epsilon 0.760000, score 47\n",
      "episode 25, epsilon 0.750000, score 13\n",
      "episode 26, epsilon 0.740000, score 12\n",
      "episode 27, epsilon 0.730000, score 29\n",
      "episode 28, epsilon 0.720000, score 14\n",
      "episode 29, epsilon 0.710000, score 29\n",
      "episode 30, epsilon 0.700000, score 41\n",
      "episode 31, epsilon 0.690000, score 32\n",
      "episode 32, epsilon 0.680000, score 26\n",
      "episode 33, epsilon 0.670000, score 26\n",
      "episode 34, epsilon 0.660000, score 15\n",
      "episode 35, epsilon 0.650000, score 18\n",
      "episode 36, epsilon 0.640000, score 19\n",
      "episode 37, epsilon 0.630000, score 39\n",
      "episode 38, epsilon 0.620000, score 66\n",
      "episode 39, epsilon 0.610000, score 33\n",
      "episode 40, epsilon 0.600000, score 31\n",
      "episode 41, epsilon 0.590000, score 36\n",
      "episode 42, epsilon 0.580000, score 65\n",
      "episode 43, epsilon 0.570000, score 14\n",
      "episode 44, epsilon 0.560000, score 81\n",
      "episode 45, epsilon 0.550000, score 51\n",
      "episode 46, epsilon 0.540000, score 29\n",
      "episode 47, epsilon 0.530000, score 41\n",
      "episode 48, epsilon 0.520000, score 115\n",
      "episode 49, epsilon 0.510000, score 104\n",
      "episode 50, epsilon 0.500000, score 64\n",
      "episode 51, epsilon 0.490000, score 101\n",
      "episode 52, epsilon 0.480000, score 31\n",
      "episode 53, epsilon 0.470000, score 78\n",
      "episode 54, epsilon 0.460000, score 34\n",
      "episode 55, epsilon 0.450000, score 26\n",
      "episode 56, epsilon 0.440000, score 129\n",
      "episode 57, epsilon 0.430000, score 79\n",
      "episode 58, epsilon 0.420000, score 96\n",
      "episode 59, epsilon 0.410000, score 78\n",
      "episode 60, epsilon 0.400000, score 35\n",
      "episode 61, epsilon 0.390000, score 150\n",
      "episode 62, epsilon 0.380000, score 276\n",
      "episode 63, epsilon 0.370000, score 129\n",
      "episode 64, epsilon 0.360000, score 160\n",
      "episode 65, epsilon 0.350000, score 178\n",
      "episode 66, epsilon 0.340000, score 147\n",
      "episode 67, epsilon 0.330000, score 206\n",
      "episode 68, epsilon 0.320000, score 239\n",
      "episode 69, epsilon 0.310000, score 218\n",
      "episode 70, epsilon 0.300000, score 222\n",
      "episode 71, epsilon 0.290000, score 156\n",
      "episode 72, epsilon 0.280000, score 256\n",
      "episode 73, epsilon 0.270000, score 231\n",
      "episode 74, epsilon 0.260000, score 220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fc1de6131bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 loss = tf.keras.losses.mean_squared_error(  # 最小化 y 和 Q-value 的距离\n\u001b[1;32m     44\u001b[0m                     \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 )\n\u001b[1;32m     47\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c06997b27e03>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')       # 实例化一个游戏环境，参数为游戏名称\n",
    "model = QNetwork()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "replay_buffer = deque(maxlen=10000) # 使用一个 deque 作为 Q Learning 的经验回放池\n",
    "epsilon = initial_epsilon\n",
    "for episode_id in range(num_episodes):\n",
    "    state = env.reset()             # 初始化环境，获得初始状态\n",
    "    epsilon = max(                  # 计算当前探索率\n",
    "        initial_epsilon * (num_exploration_episodes - episode_id) / num_exploration_episodes,\n",
    "        final_epsilon)\n",
    "    for t in range(max_len_episode):\n",
    "        env.render()                                # 对当前帧进行渲染，绘图到屏幕\n",
    "        if random.random() < epsilon:               # epsilon-greedy 探索策略，以 epsilon 的概率选择随机动作\n",
    "            action = env.action_space.sample()      # 选择随机动作（探索）\n",
    "        else:\n",
    "            action = model.predict(np.expand_dims(state, axis=0)).numpy()   # 选择模型计算出的 Q Value 最大的动作\n",
    "            action = action[0]\n",
    "\n",
    "        # 让环境执行动作，获得执行完动作的下一个状态，动作的奖励，游戏是否已结束以及额外信息\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # 如果游戏Game Over，给予大的负奖励\n",
    "        reward = -10. if done else reward\n",
    "        # 将(state, action, reward, next_state)的四元组（外加 done 标签表示是否结束）放入经验回放池\n",
    "        replay_buffer.append((state, action, reward, next_state, 1 if done else 0))\n",
    "        # 更新当前 state\n",
    "        state = next_state\n",
    "\n",
    "        if done:                                    # 游戏结束则退出本轮循环，进行下一个 episode\n",
    "            print(\"episode %d, epsilon %f, score %d\" % (episode_id, epsilon, t))\n",
    "            break\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # 从经验回放池中随机取一个批次的四元组，并分别转换为 NumPy 数组\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(\n",
    "                *random.sample(replay_buffer, batch_size))\n",
    "            batch_state, batch_reward, batch_next_state, batch_done = \\\n",
    "                [np.array(a, dtype=np.float32) for a in [batch_state, batch_reward, batch_next_state, batch_done]]\n",
    "            batch_action = np.array(batch_action, dtype=np.int32)\n",
    "\n",
    "            q_value = model(batch_next_state)\n",
    "            y = batch_reward + (gamma * tf.reduce_max(q_value, axis=1)) * (1 - batch_done)  # 计算 y 值\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.keras.losses.mean_squared_error(  # 最小化 y 和 Q-value 的距离\n",
    "                    y_true=y,\n",
    "                    y_pred=tf.reduce_sum(model(batch_state) * tf.one_hot(batch_action, depth=2), axis=1)\n",
    "                )\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))       # 计算梯度并更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37464bittf2conda24e30c6b09d04c69932e7559bed456ef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
